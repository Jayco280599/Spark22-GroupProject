<!DOCTYPE HTML>

<html>
	<head>
		<title>Server Hardware</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1>Cloud, Services and Servers - Server Hardware</h1>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="main">
								<ul class="content">
									<p>Server hardware has continually been evolving since the introduction first PC (Personal Computers) servers in the nineteen eighties. One aspect that servers have evolved exponentially is number threads of execution the CPU (Central Processing Unit) can process at once. Early servers like ordinary PC's only a had a single thread of execution with a single CPU. Introduction of SMP (Symmetrical Multiprocessing) where additional CPUs were added to enhance the processing capability of the server. Later introduction of multi-threading where each individual CPU core can execute more than one thread at time and then the introduction of multiple CPU cores per package increased the processing power density of individual servers. Today we have servers that can be purchased off the shelf boasting threads of execution numbering in the hundreds. For example, the IBM Power E980 server can be optioned up to 192 POWER 9 cores with 768 threads (four threads per core). The expansion of number of threads of execution coupled with improvements of clock speeds and IPC (Instructions Per Clock), have improved the available computational power of servers. This coupled with addition of specialized CPU instructions to enable efficient implementation of virtual machines, has helped to enable the rapid growth of cloud computing. To address the challenges associated with the coming end of <a href="https://www.intel.com.au/content/www/au/en/silicon-innovations/moores-law-technology.html">Moore's Law</a>, companies like AMD are looking to move away from monolithic CPU dies to chip-let type CPU packaging design. This separates the CPU and their associated caches into a separate die from the I/O (Input/Output) components (e.g., PCI controller, Memory controller, etc...), which allow high performance CPU chip-lets to be produced on the latest silicon fabrication node to improve efficiency, while the less demanding I/O controller die can be still fabricated on previous generation process node. This approach allows for smaller dies which make more efficient use of circular silicon wafer area and avoids potential waste that can affect larger monolithic dies. One potential benefit of the chip-let design approach that may be taken by intel, is the use of third-party processor or accelerator dies on the processor package. This approach can be used to tailor the CPU package for specific compute workloads such as AI (Artificial Intelligence), deep-learning or HPE (High Performance Computing) workloads, depending on the customers' requirements. This approach has the potential to improve performance and efficiency of the processors in various workloads, while overcoming the limitations being encountered with Moore's Law. This will be a major consideration going forward with the concerns of the ever-increasing energy consumption of data centres and their potential environmental impacts. One emerging technology slowing entering the data centre that could change the power efficiency dynamic, is the ARM (Advanced RISC Machines) architecture. First finding widespread use in mobile devices such as smartphones and tablets due its major advantage of providing satisfactory performance with exceedingly low power consumption, ARM processors have been steadily increasing in processing power. Though adoption of ARM based systems in the datacentre has been slow, it has steadily gaining a foothold to a point that AWS (Amazon Web Services) now even provide ARM based VMs (Virtual Machine) instances. ARM processors have developed to the point that Apple themselves have now started to use ARM based processors, of their own design, in their laptop and desktop offerings. ARM adoption in the datacentre will continue to grow with further software support being provided by enterprise software providers. Another future disruptor in the datacentre is RISC-V. Originally created at the University of California, Berkley, RISC-V is open-source ISA (Instruction Set Architecture) that manufactures can use without the need to pay royalties or licensing fees, which is the case with ARM and x86 architectures. The RISC-V architecture is attractive to companies who are interested in developing their own custom silicon, while removing the need to develop an entire ISA and software toolchain from scratch. RISC-V is already supported by major open-source operating systems such as Linux, FreeBSD and NetBSD. With support for RISC-V platform growing, it has the potential to be a future big player in the CPU space in the coming decade, alongside the likes of ARM and x86.
									<p>Computer memory has come along in leaps and bounds over time since the first DRAM (Dynamic Random-Access Memory) chips hit the market in the seventies. Memory capacities and performance have increased dramatically over the past couple of decades. The addition of ECC (Error Correcting Code) technology to server and workstation memory has improved reliability of systems, reducing the in-flight corruption that can bring down systems. The introduction of DDR (Double Data Rate) technology and multi-channel memory controllers have improved the available bandwidth and therefore the performance of the system. Whilst computer memory has evolved consistency with other technologies, one interesting development introduced by intel has put an interesting spin on what is memory. Traditionally memory, though limited in size compared but faster than storage (SSD's and hard drives) and slower but larger capacity compared to on die CPU caches, memory has been a high-performance middle ground that software applications can reside and work when running. Intel Optane Memory has brought an interesting development to fore, persistent memory. Typically, in computer systems, memory is considered volatile, meaning what is stored in memory is lost when either the system is rebooted or powered off. Intel Optane is persistent memory, or non-volatile, which means it retains its contents across reboots and even when it powered off. Though other non-volatile memory technologies are currently such as NVMe (Non-Volatile Memory), but NVMe is typically interfaced though the PCIe controller. Intel Optane Memory on other hand, is interfaced directly through the CPU memory controller itself. Intel Optane Memory is different from ordinary flash memory, in that it uses technology called 3D XPoint, which make Optane faster than ordinary flash memory but more expensive to produce. Though persistent memory like Optane is slower than conventional memory, it does offer greater capacities than conventional memory. This allows Optane to be used as a high-performance cache between storage and memory, to enhance performance in IO (Input/Output) bound applications. Intel Optane Memory can be also used to add resilience to database servers without sacrificing performance, where changes to the database are cached in Optane Memory before being written to disk. This allows the database to move to other operations without waiting data to be written to storage, but the integrity of the database can be ensured as Optane Memory is persistent, even in the event of power loss or restart. The main factor that is limiting the uptake of the intel Optane Memory technology, is that intel is currently the only vendor offering it. This has helped intel to remain competitive in the datacentre, even in face of stiff competition from AMD's EYPC and ARM based servers. Wide-spread adoption of the technology will be limited until other vendors such as AMD, ARM, RISC-V, and IBM (Power) are able to start offering a similar technology in their systems.
									<p>Storage has come a long way from the days of paper tape and punched cards. Ever since the introduction of the first hard drive by IBM in the fifties, which had fifty-two platers of 61cm in diameter and total capacity of 3.75MB, hard drives have continued to increase in capacity and performance, while reducing in cost and size. In a mere three decades, hard drives went from capacities measured in megabytes and costing many hundreds or thousands of dollars to now being measured in terabytes at a fraction of the cost. Today, even though they have been eclipsed by better performing technologies, hard drives still rule the roost when it comes to cost of storage. To help improve performance and/or redundancy of hard drives over time, technologies like SCSI (Small Computer System Interface) and RAID (Redundant Array of Inexpensive Disks) where developed and widely used in the professional workstations and server market (many early Apple MacIntosh systems used SCSI). Big leap of performance came in the form of the SSD (Solid State Drive). Not being reliant on mechanical means of reading and writing data, removed the many physical limitations that have plagued hard drives that have limited drive performance. Though some high performing enterprise hard drives may come close sequential data throughput, SSD's, with their lack of mechanical machinery, have far superior random read/write performance. SSD technology have brought significant performance benefit to the computing space all the while delivering substantial power savings in the process. This has many advantages in the mobile space. Despite the advantages of SSD's compared to hard drives, SSD's still lag substantially behind hard drives in storage capacity and cost of storage. The introduction of TLC (Triple-Level Cell), MLC (Multi-Level Cell) and QLC (Quad-Level Cell) flash memory technologies have improved storage capacities while reducing their cost, but SSD's still lags considerably behind hard drives in terms of cost of storage. Another limitation of SSDss is that they have a limited write lifespan when compared to hard drives (especially with QLC drives), this a physical limit of number of writes the drive can perform. Vendors do use mitigation strategies to reduce the impact of the limitation such as write balancing and having reserved data blocks as replacements for data blocks that have reached this limit. The large storage arrays in data centres still use hard drives as primary form of large-scale storage, and this is unlikely to change soon. SSD storage has become more mainstream storage in desktop and mobile sectors where their performance and power consumption have clear advantages over hard drives, despite their limited size. SSD storage is also commonplace in servers as they are used as boot drives, where drive capacity is less important and in mass storage where performance is paramount, as in some database applications. SSD storage will eventually overtake hard drives as drive capacities increase, decreases in costs, and improvements to write lifespan as improvements in manufacturing processes come about. The days of the hard drive are coming to an end, as they too are hitting the limits of magnetic recording technologies and increasing hard drive capacities are having diminishing returns.
								</ul>
								<ul class="actions">
									<li><a href="index.html" class="button">Go Back</a></li>
								</ul>
							</section>

					</div>

				
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>